{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hFcZFYIqEv1"
      },
      "outputs": [],
      "source": [
        "!pip install requests beautifulsoup4\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37P-q1W1YNgM"
      },
      "source": [
        "**FOR HUSNAIN 1/8/24:**\n",
        "I deleted the segment of code that removes single element classes. That however does not mean that the single element classes will be included in the filtered dataset since we created that dataset off of manually picking the classes. We probably ignored a majority of the classes that only had one entry at the time of manually picking so if you want to include single element classes in the filtered dataset, you'll have to go through the ascii page again and reselect a few classes that you and I skipped over. I'll be happy to help with this, just lmk.\n",
        "\n",
        "I also updated a few of the filtering methods and that's really the only difference/changes i made.\n",
        "\n",
        "focus on finding a way to take the filtered dataset and change the names of the objects in the JSON so that the class that they belong to aren't abstract (for example air or small). You can either do that by 1. doing some post processing and importing the json and running some sort of algorithm to change the class names of any object that has a vague class name or 2. doing pre-processing and changing the names of those objects before they are saved to a json as a database. If you wanna go with option 2, i suggest looking at the function\n",
        "def save_ascii_art(url, metadata, base_dir):\n",
        "and find a solution there.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hltsjdorrN7p"
      },
      "outputs": [],
      "source": [
        "!pip install requests beautifulsoup4 pytesseract Pillow\n",
        "!apt install tesseract-ocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2q5gOIzpJtZ_"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers torch\n",
        "# !pip install gensim\n",
        "# !pip install scikit-learn\n",
        "# !pip install umap-learn\n",
        "# !pip install matplotlib\n",
        "# !pip install seaborn\n",
        "# !pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wEDsybjpKQJ"
      },
      "outputs": [],
      "source": [
        "# # Define the categories and their respective classes\n",
        "# import torch\n",
        "# from transformers import RobertaTokenizer, RobertaModel\n",
        "# import numpy as np\n",
        "\n",
        "# # Load RoBERTa tokenizer and model\n",
        "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "# model = RobertaModel.from_pretrained('roberta-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJjm6opJTQ1e"
      },
      "outputs": [],
      "source": [
        "# # Function to generate embeddings for a text using RoBERTa\n",
        "# def get_roberta_embedding(text):\n",
        "#     # Tokenize and convert to tensor\n",
        "#     inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "#     # Generate embeddings\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(**inputs)\n",
        "\n",
        "\n",
        "#     # Mean pooling - Aggregate token embeddings to get a single vector for the text\n",
        "#     return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "# # Your categories dictionary\n",
        "# categories = {\n",
        "#     \"ANIMALS\": [\"aardvark\", \"bear\", \"bird\", \"butterfly\", \"camel\", \"cat\", \"crab\", \"dinosaur\", \"deer\", \"dog\", \"bat\", \"dolphin\", \"dragon\", \"ducks\", \"eagle\", \"elephant\", \"fox\", \"frog\", \"giraffe\", \"goat\", \"goose\", \"insect\", \"kangaroo\", \"lion\", \"lobster\", \"moose\", \"moth\", \"octopus\", \"orca\", \"ostrich\", \"owl\", \"panda\", \"panther\", \"peacock\", \"pelican\", \"penguins\", \"pheonix\", \"pig\", \"pigeon\", \"puppy\", \"zebra\", \"wolf\", \"whale\", \"wasp\", \"walrus\", \"turtle\", \"unicorn\", \"stegosaurus\", \"stag\", \"squirrel\", \"spider\", \"spaniel\", \"snail\", \"sharks\", \"stork\", \"swan\", \"sheep\", \"stingray\", \"seahorse\", \"scorpion\", \"rooster\", \"reindeer\", \"raven\", \"clam\", \"horse\", \"yak\", \"worm\", \"toucan\", \"tortoise\", \"seal\", \"robin\", \"rhino\"],\n",
        "#     \"ACTIVITIES\": [\"yoga\", \"yawn\", \"dancing\", \"wrapping\", \"weightlifting\", \"volleyball\", \"tapdancing\", \"skijumping\", \"skiing\", \"rowing\", \"reading\", \"football\", \"write\", \"soccer\", \"tennis\", \"sports\", \"snorkel\", \"shopping\", \"shoot\", \"sewing\", \"scream\", \"sail\", \"running\", \"rollercoaster\", \"recycle\", \"scuba\"],\n",
        "#     \"FICTIONAL CHARACTERS\": [\"batman\", \"angel\", \"x-men\", \"cyberpunk\", \"ghost\", \"yoda\", \"wizard\", \"witch\", \"winniethepooh\", \"wallacegromit\", \"vampire\", \"rapunzel\", \"starwars1\", \"startrek1\", \"spiderman\", \"snoopy\", \"sleipner\", \"sisyphus\", \"simba\", \"sherlockholmes\", \"sesamestreet\", \"romeoandjuliet\", \"roadrunner\", \"stallone\"],\n",
        "#     \"FOOD\": [\"watermelon\", \"apple\", \"grapes\", \"mushroom\", \"wine\", \"tea\", \"sandwich\", \"coffee\", \"vitamins\", \"toast\", \"soup\", \"tomato\"],\n",
        "#     \"LOCATIONS\": [\"zoo\", \"bedrooms\", \"africa\", \"city\", \"house\", \"island\", \"usa\", \"wales\", \"uk\", \"turkey\", \"shops\", \"room\", \"restaurant\", \"land\",  \"world\", \"whistle\", \"waterfall\", \"volcano\", \"villages\", \"treehouse\", \"treasureisland\", \"town\", \"tajmahal\", \"street\", \"stonehenge\", \"spaceshuttle\",  \"skyscraper\", \"samoa\"],\n",
        "#     \"EVENTS\": [\"worldcup\", \"xmas\", \"winter\", \"wedding\", \"valentine\"],\n",
        "#     \"MODES OF TRANSPORTATION\": [\"helicopter\", \"plane\" ,\"boat\", \"car\", \"air\", \"airplane\", \"yacht\", \"bus\", \"vw\", \"volkswagen\", \"truck\", \"titanic\", \"tram\", \"train\", \"spaceships\", \"ships\", \"scooter\", \"sailplane\",  \"sailboat\", \"rollerskates\", \"roadworks\", \"waterski\", \"jeep\", \"skateboard\", \"motorcycles\", \"submarine\", \"x-wing\", \"unicycles\", \"zeppelin\"],\n",
        "#     \"TECHNOLOGY\": [\"toaster\", \"tv\",\"computer\",\"keyboard\",\"laptop\", \"phone\",\"satellites\", \"robot\", \"camera\", \"oven\", \"telephone\", \"printer\", \"clock\", \"radiotelescope\", \"thermometer\", \"macintosh\", \"ufo\", \"lock\", \"windmill\", \"watch\", \"lighter\", \"telescope\",  \"stirlingengine\", \"stove\", \"waterwell\"],\n",
        "#     \"OBJECTS\": [\"bell\", \"candle\", \"dice\", \"disk\", \"globe\", \"postcard\",  \"wonderlamp\", \"wok\", \"windows\", \"web\", \"umbrella\", \"trophy\", \"towel\", \"treasure\", \"tents\", \"tape\",  \"stamps\", \"sphere\", \"soap\", \"snowglobe\",  \"snowball\", \"snowflakes\", \"sledge\",  \"sign\", \"shield\", \"scroll\", \"scales\", \"scarecrow\", \"saw\", \"sack\", \"rockinghorse\",\"ring\"],\n",
        "#     \"MISCELLANEOUS\": [\"couples\", \"zodiac\", \"words\", \"why\", \"who\", \"warnerbros\",  \"walls\", \"vangogh\", \"tutorial\",  \"quiet\", \"please\",  \"rainbow\", \"traffic\", \"toystory\",  \"tolkien\", \"tictactoe\", \"thumbsup\", \"sunset\",  \"stop\", \"splat\", \"smile\", \"small\", \"skyline\", \"skijumper\", \"sinks\", \"sink\", \"singer\", \"sine\", \"sillywalks\", \"shadow\", \"science\", \"abacus\", \"ring\"],\n",
        "#     \"STATIONARY\": [\"books\", \"pen\", \"quill\"],\n",
        "#     \"CLOTHES\": [\"clothes\", \"shoes\", \"boots\"],\n",
        "#     \"NATURE\": [\"acorn\", \"mountain\",  \"daisies\", \"tree\", \"palmtree\", \"tulips\", \"thistle\", \"blossom\", \"clouds\", \"lightning\", \"wind\", \"tornado\", \"shamrock\", \"rose\", \"shrubbery\"],\n",
        "#     \"STRUCTURES\": [\"pyramid\",\"watchtower\", \"wells\", \"bridge\", \"sphinx\", \"towers\", \"tower\", \"whitetower\", \"castle\", \"gate\"],\n",
        "#     \"FEELINGS\": [\"romance\", \"relax\"],\n",
        "#     \"INSTRUMENTS\": [\"turntables\", \"trombone\", \"trumpet\", \"violin\",\"recordplayer\", \"sax\"],\n",
        "#     \"BODY PARTS\": [\"mouth\", \"eye\", \"face\", \"brain\", \"hand\", \"head\", \"heart\"],\n",
        "#     \"TOOLS\": [\"tools\", \"toolbox\", \"key\", \"keys\", \"scissors\"],\n",
        "#     \"WEAPONS\": [\"axe\",\"dagger\", \"knife\", \"gun\", \"raygun\"],\n",
        "#     \"REAL PEOPLE\": [\"fighter\", \"astronaut\", \"boy\", \"clown\", \"girl\", \"joker\", \"knight\", \"pirate\", \"police\", \"surfer\", \"warrior\", \"samurai\"],\n",
        "#     \"FURNITURE\": [\"stairs\", \"bed\", \"doors\", \"stool\", \"sofas\", \"lamp\", \"settee\"],\n",
        "#     \"CONTAINER\": [\"basket\", \"bottle\", \"bucket\", \"teapot\"],\n",
        "#     \"DECORATION\": [\"diamond\", \"gems\", \"wreath\"],\n",
        "#     \"OCEAN\": [\"shell\", \"seashell\", \"wave\",  \"seascape\", \"sea\"],\n",
        "#     \"UNIVERSE\": [\"sun\", \"solarsystem\", \"saturn\", \"moon\", \"planets\"]\n",
        "# }\n",
        "\n",
        "# # Prepare representative text data\n",
        "# representative_texts = {category: ' '.join(classes).lower() for category, classes in categories.items()}\n",
        "\n",
        "# # Generate embeddings for each category\n",
        "# category_embeddings = {category: get_roberta_embedding(text) for category, text in representative_texts.items()}\n",
        "\n",
        "# from sklearn.manifold import TSNE\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Prepare embeddings for T-SNE\n",
        "# embeddings_list = np.array(list(category_embeddings.values()))\n",
        "# category_labels = list(category_embeddings.keys())\n",
        "\n",
        "# # Number of categories\n",
        "# n_categories = len(embeddings_list)\n",
        "\n",
        "# # Apply T-SNE with adjusted perplexity\n",
        "# tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, max(5, n_categories // 3)))\n",
        "# reduced_embeddings = tsne.fit_transform(embeddings_list)\n",
        "\n",
        "\n",
        "# # Apply T-SNE\n",
        "# reduced_embeddings = tsne.fit_transform(embeddings_list)\n",
        "\n",
        "# # Visualization\n",
        "# plt.figure(figsize=(12, 8))\n",
        "# for i, label in enumerate(category_labels):\n",
        "#     x, y = reduced_embeddings[i, 0], reduced_embeddings[i, 1]\n",
        "#     plt.scatter(x, y)\n",
        "#     plt.annotate(label, (x, y), textcoords=\"offset points\", xytext=(5,5), ha='center')\n",
        "# plt.xlabel('TSNE Component 1')\n",
        "# plt.ylabel('TSNE Component 2')\n",
        "# plt.title('T-SNE Visualization of ASCII Art Categories')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHSp6s3jovRy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhSKFfN5ri-E"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from PIL import Image\n",
        "import os\n",
        "import pytesseract\n",
        "import io\n",
        "import re\n",
        "from google.colab import drive  #for Google Drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kxot-5dM2GnV"
      },
      "outputs": [],
      "source": [
        "# def split_ascii_art(pre_tag_content):\n",
        "#     # Split the ASCII art into separate images based on patterns of multiple newlines\n",
        "#     # Adjust the pattern as necessary based on the specific formatting of your ASCII art\n",
        "#     ascii_arts = re.split(r'\\n{2,}', pre_tag_content.strip())\n",
        "#     return ascii_arts\n",
        "\n",
        "\n",
        "def split_pre_tag_content(pre_tag_content):\n",
        "    # Use a regular expression to split the content on ', ' or ', \"'\n",
        "    # The pattern looks for occurrences of ', ' or ', \"' and splits the string at these points.\n",
        "    split_contents = re.split(r'\\n\\s*\\n+', pre_tag_content)\n",
        "    return split_contents\n",
        "\n",
        "#Case 1-there isn't a single other alphabetical character other than the 3 letter artist tag, and the artist tag is removed\n",
        "def replace_alpha_with_space(art):\n",
        "    # Replace up to 3 alphabetic characters with spaces\n",
        "    return re.sub('[a-zA-Z]', ' ', art, count=3)\n",
        "\n",
        "def classify_and_modify_ascii_art(ascii_arts):\n",
        "    with_alpha = []\n",
        "    without_alpha = []\n",
        "\n",
        "    for art in ascii_arts:\n",
        "        # Count the number of alphabetic characters in the ASCII art\n",
        "        alpha_count = len(re.findall('[a-zA-Z]', art))\n",
        "\n",
        "        # Classify based on the count of alphabetic characters\n",
        "        if alpha_count > 3:\n",
        "            with_alpha.append(art)\n",
        "        else:\n",
        "            modified_art = replace_alpha_with_space(art)\n",
        "            #Case 1\n",
        "            without_alpha.append(modified_art)\n",
        "\n",
        "\n",
        "    return with_alpha, without_alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bRvng9YEao2"
      },
      "outputs": [],
      "source": [
        "#Case 2-there is a artist tag, and other alphabetical characters in the image:\n",
        "#  a pattern of three consecutive characters is detected, adding one to a count, and if\n",
        "#  the count doesn't go over 1, the 3 consecutive characters are removed\n",
        "def remove_artist_tag(ascii_art, tag_area_height=3):\n",
        "    # Split the ASCII art into lines\n",
        "    lines = ascii_art.split('\\n')\n",
        "\n",
        "    # Assuming the artist tag is within the last few lines of the art\n",
        "    tag_area_lines = lines[-tag_area_height:]\n",
        "\n",
        "    # Pattern for two or three consecutive alphabetical characters at the end of a line\n",
        "    pattern = r'[A-Za-z]{2,3}$'\n",
        "\n",
        "    # Function to replace the matched pattern with spaces equal to its length\n",
        "    def replace_with_spaces(match):\n",
        "        return ' ' * len(match.group())\n",
        "\n",
        "    # Modify only the last few lines where the tag is likely to be\n",
        "    modified_tag_area_lines = [re.sub(pattern, replace_with_spaces, line) for line in tag_area_lines]\n",
        "\n",
        "    # Reconstruct the ASCII art with the modified tag area\n",
        "    modified_ascii_art = '\\n'.join(lines[:-tag_area_height] + modified_tag_area_lines)\n",
        "\n",
        "    return modified_ascii_art"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKI0E_0-BvQB"
      },
      "outputs": [],
      "source": [
        "#Case 3-there is a artist tag, and other alphabetical characters in the image, and the artist tag is on the right side of the image:\n",
        "#Scan for artist tags that are surrounded by white space on the left, and an endline character on the right\n",
        "def remove_right_tag(ascii_art):\n",
        "    # The regular expression looks for 1 or more spaces (\\s+), followed by 2-3 alphabetical characters ([a-zA-Z]{2,3}),\n",
        "    # followed by optional 1-3 spaces (' {0,3}'), and then a newline character (\\n)\n",
        "    pattern = r'(\\s+)[a-zA-Z]{2,3}( {0,3})\\n'\n",
        "\n",
        "    # Replace the found pattern with spaces (keeping the same number of spaces and newline)\n",
        "    # The lambda function in re.sub allows us to keep the same number of spaces as in the original match\n",
        "    modified_art = re.sub(pattern, lambda match: ' ' * (len(match.group(0)) - 1) + '\\n', ascii_art)\n",
        "\n",
        "    return modified_art"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dTavmwHx4gG"
      },
      "outputs": [],
      "source": [
        "#Case 4: webpage doesn't have more than one entry: most likely a shitpost of sorts (eg. \"blonde webpage literally just has a face that says 'where is that brain of mine?'\")\n",
        "#ACCOUNTED FOR IN SCRAPE_AND_OCR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uRfkaZgx51p"
      },
      "outputs": [],
      "source": [
        "#Case 5: creator tag is literally \"unknown\"\n",
        "def remove_unknown_artist_tag(ascii_art):\n",
        "    # Regular expression pattern to find the word 'unknown'\n",
        "    pattern = r'\\bunknown\\b'\n",
        "\n",
        "    # Replace 'unknown' with spaces\n",
        "    modified_art = re.sub(pattern, '       ', ascii_art)  # 7 spaces for the length of 'unknown'\n",
        "\n",
        "    return modified_art"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szua43wdyvSP"
      },
      "outputs": [],
      "source": [
        "#Case 6: creator tag is on the left; within 0-2 white spaces on the left occurs a newline character, and 2+ white spaces on the right\n",
        "def remove_creator_tag_pattern(ascii_art):\n",
        "    # Pattern for newline, 1-2 spaces, a 2-4 letter word, and 2+ spaces\n",
        "    pattern = r'\\n\\s{1,2}[A-Za-z]{2,4}\\s{2,}'\n",
        "\n",
        "    # Find the first match\n",
        "    if re.search(pattern, ascii_art):\n",
        "        # Replace the first occurrence of the found pattern\n",
        "        # The replacement length is calculated based on the match length\n",
        "        match = re.search(pattern, ascii_art)\n",
        "        replacement_length = len(match.group()) - 1  # Subtract 1 for the newline character\n",
        "        ascii_art = re.sub(pattern, '\\n' + ' ' * replacement_length, ascii_art, count=1)\n",
        "\n",
        "    return ascii_art"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rI5v9oo0Cvw"
      },
      "outputs": [],
      "source": [
        "#Case 7: divider between images: a bunch of \"-\" in a row or \"_\" in a row.\n",
        "def remove_divider_of_ascii_image(ascii_image):\n",
        "    # Split the ASCII image into lines\n",
        "    lines = ascii_image.split('\\n')\n",
        "\n",
        "    # Define the pattern to search for: series of \"-\" or \"_\"\n",
        "    pattern = r'[-_]+'\n",
        "\n",
        "    # Replace the detected patterns with spaces\n",
        "    modified_lines = [re.sub(pattern, lambda m: ' ' * len(m.group()), line) for line in lines]\n",
        "\n",
        "    # Combine the modified lines back into a single string\n",
        "    modified_image = '\\n'.join(modified_lines)\n",
        "\n",
        "    return modified_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbbR3EgO0KkS"
      },
      "outputs": [],
      "source": [
        "#Case 8: a date is included. EG. 12/21/2023 or 21nov2023 or dec21 23 or 12.21.2023 etc.\n",
        "def remove_dates_from_ascii_art(ascii_art):\n",
        "    \"\"\"\n",
        "    Removes dates in various formats from the ASCII art.\n",
        "    It targets a wide range of date formats.\n",
        "    \"\"\"\n",
        "    # Regular expression patterns to find various date formats\n",
        "    date_patterns = [\n",
        "        r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b',              # Matches dates like 11/23/2023 or 11-23-2023\n",
        "        r'\\b\\d{1,2}[.]\\d{1,2}[.]\\d{2,4}\\b',                # Matches dates like 11.23.2023\n",
        "        r'\\b\\d{1,2}[a-zA-Z]{3}\\d{2,4}\\b',                  # Matches dates like 11nov2023\n",
        "        r'\\b\\d{1,2}-[a-zA-Z]{3}-\\d{2,4}\\b',                # Matches dates like 11-nov-2023\n",
        "        r'\\b\\d{1,2}\\s[a-zA-Z]{3}\\s\\d{2,4}\\b',              # Matches dates like 11 nov 2023\n",
        "        r'\\b[a-zA-Z]{3}\\s\\d{1,2},\\s\\d{4}\\b',               # Matches dates like Nov 11, 2023\n",
        "        r'\\b\\d{4}[/-]\\d{1,2}[/-]\\d{1,2}\\b',                # Matches dates like 2023/11/23 or 2023-11-23 (ISO format)\n",
        "        r'\\b\\d{1,2}[a-zA-Z]{3}\\b',                         # Matches shorter dates like 11nov\n",
        "        r'\\b\\d{2}[/-]\\d{2}[/-]\\d{2}\\b',                    # Matches shorter dates like 11/23/23 or 11-23-23\n",
        "        r'\\b[a-zA-Z]{3,9}\\s\\d{1,2},?\\s\\d{4}\\b'\n",
        "    ]\n",
        "\n",
        "    modified_art = ascii_art\n",
        "    for pattern in date_patterns:\n",
        "        modified_art = re.sub(pattern, lambda match: ' ' * len(match.group(0)), modified_art)\n",
        "\n",
        "    return modified_art"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbYWkrgJF_Jt"
      },
      "outputs": [],
      "source": [
        "#Case 9: email detected\n",
        "def replace_emails_in_ascii_image(ascii_image_string):\n",
        "    # Find all email addresses using regex\n",
        "    emails = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', ascii_image_string)\n",
        "\n",
        "    # Replace each email with spaces\n",
        "    for email in emails:\n",
        "        ascii_image_string = ascii_image_string.replace(email, ' ' * len(email))\n",
        "\n",
        "    return ascii_image_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vFzsEZX1Mci"
      },
      "outputs": [],
      "source": [
        "#Case 10: 3 letter creator tag surrounded by either dashes or brackets(\"--\" or \"[]\")\n",
        "def remove_bracket_or_dash_enclosed_tags(ascii_art):\n",
        "    \"\"\"\n",
        "    Removes 2-3 letter creator tags surrounded by either dashes ('--') or brackets ('[]').\n",
        "    \"\"\"\n",
        "    # Corrected regular expression pattern to find 2-3 letter tags enclosed in dashes or brackets\n",
        "    pattern = r'(--[A-Za-z]{2,3}--)|(\\[[A-Za-z]{2,3}\\])'\n",
        "\n",
        "    # Replace the pattern with spaces or remove it\n",
        "    def replace_with_spaces(match):\n",
        "        return ' ' * len(match.group())\n",
        "\n",
        "    # Replace the pattern with equivalent spaces\n",
        "    modified_art = re.sub(pattern, replace_with_spaces, ascii_art)\n",
        "\n",
        "    return modified_art"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXS4K_WBY9gE"
      },
      "outputs": [],
      "source": [
        "BLACKLIST=[\"jrei\", \"jgs\", \"VK\", \"DUTCH\", \"DUDE\", \"ejm97\", \"JRO\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cs4LdS-W1i4r"
      },
      "outputs": [],
      "source": [
        "#Case 11: remove certain creator tags\n",
        "# THIS IS THE CODE FOR CASE 11:\n",
        "\n",
        "import re\n",
        "\n",
        "def remove_creator_tags(ascii_art, blacklist):\n",
        "    \"\"\"\n",
        "    Removes creator tags specified in the blacklist from the ASCII art.\n",
        "    These tags may appear in various positions and possibly within brackets or other enclosures.\n",
        "    The removed tags are replaced with the same number of spaces to preserve the layout of the ASCII art.\n",
        "    \"\"\"\n",
        "    # Escape special characters in the tags and create a combined regular expression pattern\n",
        "    escaped_tags = [re.escape(tag) for tag in blacklist]\n",
        "    tag_pattern = r'\\b(' + '|'.join(escaped_tags) + r')\\b|\\[\\s*(' + '|'.join(escaped_tags) + r')\\s*\\]'\n",
        "\n",
        "    # Custom replacement function\n",
        "    def replace_with_spaces(match):\n",
        "        return ' ' * len(match.group(0))\n",
        "\n",
        "    # Replace found patterns using the custom replacement function\n",
        "    modified_art = re.sub(tag_pattern, replace_with_spaces, ascii_art)\n",
        "\n",
        "    return modified_art"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypGODSRX68ZZ"
      },
      "outputs": [],
      "source": [
        "#Case 12: the array element is 2 lines or less.\n",
        "def filter_by_line_count(ascii_arts):\n",
        "    return [art for art in ascii_arts if len(art.split('\\n')) >= 3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjmkWOWVFMcV"
      },
      "outputs": [],
      "source": [
        "#IMPLEMENT THIS\n",
        "#np and .split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDWB6Wqe8AHq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2DehBQS4ObF"
      },
      "outputs": [],
      "source": [
        "def extract_urls(url, start_value, end_value):\n",
        "    # Send a request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code != 200:\n",
        "        return f\"Error: The request returned a status code {response.status_code}\"\n",
        "\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find all anchor tags\n",
        "    anchor_tags = soup.find_all('a')\n",
        "\n",
        "    # Extract URLs from each anchor tag and filter based on the name\n",
        "    filtered_urls = []\n",
        "    for tag in anchor_tags:\n",
        "        href = tag.get('href')\n",
        "        if href and href.startswith('/art/'):\n",
        "            # Extract the relevant part of the path\n",
        "            name_part = href[5:]  # Assuming all paths start with '/art/'\n",
        "\n",
        "            # Ensure that name_part is not empty\n",
        "            if name_part:\n",
        "                # Get the first character of the name part, in lowercase\n",
        "                first_char = name_part[0].lower()\n",
        "\n",
        "                # Check if the first character is within the specified range\n",
        "                if start_value <= first_char <= end_value:\n",
        "                    filtered_urls.append(href)\n",
        "\n",
        "    return filtered_urls\n",
        "\n",
        "\n",
        "# Example usage\n",
        "urls = extract_urls('https://ascii.co.uk/art', 'a', 'z')\n",
        "for url in urls:\n",
        "    print(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jscpfRQ-uxuC"
      },
      "outputs": [],
      "source": [
        "classes= [\"aardvark\", \"abacus\", \"acorn\", \"africa\", \"air\", \"airplane\", \"angel\", \"apple\",\n",
        "    \"astronaut\", \"axe\", \"basket\", \"bat\", \"batman\", \"bear\", \"bed\", \"bedrooms\", \"bell\", \"bird\",\n",
        "    \"birds\", \"blossom\", \"boat\", \"books\", \"boots\", \"bottle\", \"boy\", \"brain\", \"bridge\", \"bucket\",\n",
        "    \"bus\", \"butterfly\", \"camel\", \"camels\", \"camera\", \"candle\", \"car\", \"castle\", \"cat\", \"cats\",\n",
        "    \"city\", \"clam\", \"clock\", \"clothes\", \"clouds\", \"clown\", \"coffee\", \"computer\", \"couples\", \"crab\",\n",
        "    \"cyberpunk\", \"dagger\", \"daisies\", \"dancing\", \"diamond\", \"deer\", \"dice\", \"dinosaur\", \"disk\",\n",
        "    \"dog\", \"dolphin\", \"dragon\", \"doors\", \"ducks\", \"eagle\", \"elephant\", \"eye\", \"face\", \"fighter\",\n",
        "    \"football\", \"fox\", \"frog\", \"gate\", \"gems\", \"ghost\", \"giraffe\", \"girl\", \"globe\", \"goat\", \"goose\",\n",
        "    \"grapes\", \"gun\", \"hand\", \"head\", \"heart\", \"helicopter\", \"horse\", \"house\", \"insect\", \"island\",\n",
        "    \"jeep\", \"joker\", \"kangaroo\", \"key\", \"keyboard\", \"keys\", \"knife\", \"knight\", \"lamp\", \"land\",\n",
        "    \"laptop\", \"lightning\", \"lighter\", \"lion\", \"lions\", \"lobster\", \"lock\", \"macintosh\", \"moose\",\n",
        "    \"moon\", \"moth\", \"motorcycles\", \"mountain\", \"mouth\", \"mushroom\", \"octopus\", \"orca\", \"ostrich\",\n",
        "    \"oven\", \"owl\", \"palmtree\", \"panda\", \"panther\", \"peacock\", \"pen\", \"pelican\", \"penguins\", \"pheonix\",\n",
        "    \"phone\", \"pig\", \"pigeon\", \"pigs\", \"pirate\", \"plane\", \"planets\", \"police\", \"postcard\", \"printer\",\n",
        "    \"puppy\", \"pyramid\", \"zoo\", \"zodiac\", \"zeppelin\", \"zebra\", \"yoga\", \"yoda\", \"yawn\", \"yak\", \"yacht\",\n",
        "    \"x-wing\", \"x-men\", \"xmas\", \"write\", \"wreath\", \"wrapping\", \"worm\", \"worldcup\",\n",
        "    \"world\", \"words\", \"wonderlamp\", \"wolf\", \"wok\", \"wizard\", \"witch\", \"winter\",\n",
        "    \"winniethepooh\", \"wine\", \"windows\", \"windmill\", \"wind\", \"why\", \"who\", \"whitetower\",\n",
        "    \"whistle\", \"whale\", \"wells\", \"weightlifting\", \"wedding\", \"web\", \"wave\", \"waterwell\",\n",
        "    \"waterski\", \"watermelon\", \"waterfall\", \"watchtower\", \"watch\", \"wasp\", \"warrior\",\n",
        "    \"warnerbros\", \"walrus\", \"walls\", \"wallacegromit\", \"wales\", \"vw\", \"volcano\", \"volleyball\",\n",
        "    \"volkswagen\", \"volcano\", \"vitamins\", \"violin\", \"villages\", \"vangogh\", \"vampire\",\n",
        "    \"valentine\", \"usa\", \"unicycles\", \"unicorn\", \"umbrella\", \"uk\", \"ufo\", \"tv\", \"tutorial\",\n",
        "    \"turtle\", \"turntables\", \"turkey\", \"tulips\", \"trumpet\", \"truck\", \"trophy\", \"trombone\",\n",
        "    \"treehouse\", \"tree\", \"treasureisland\", \"treasure\", \"tram\", \"train\", \"traffic\", \"toystory\",\n",
        "    \"town\", \"towers\", \"tower\", \"towel\", \"toucan\", \"tortoise\", \"tornado\", \"tools\", \"toolbox\",\n",
        "    \"tomato\", \"tolkien\", \"toaster\", \"toast\", \"titanic\", \"tictactoe\", \"thumbsup\", \"thistle\",\n",
        "    \"thermometer\", \"tents\", \"tennis\", \"telescope\", \"telephone\", \"teapot\", \"tea\", \"tape\",\n",
        "    \"tapdancing\", \"tajmahal\", \"swan\", \"surfer\", \"sunset\", \"sun\", \"submarine\", \"street\",\n",
        "    \"stove\", \"stork\", \"stop\", \"stool\", \"stonehenge\", \"stirlingengine\", \"stingray\", \"stegosaurus\",\n",
        "    \"starwars1\", \"startrek1\", \"stamps\", \"stallone\", \"stairs\", \"stag\", \"squirrel\", \"sports\",\n",
        "    \"splat\", \"spiderman\", \"spider\", \"sphinx\", \"sphere\", \"spaniel\", \"space shuttle\", \"spaceships\",\n",
        "    \"soup\", \"solarsystem\", \"sofas\", \"soccer\", \"soap\", \"snowglobe\", \"snoopy\", \"snorkel\",\n",
        "    \"snowball\", \"snowflakes\", \"snail\", \"smile\", \"small\", \"sleipner\", \"sledge\", \"skydiver\",\n",
        "    \"skyline\", \"skyscraper\", \"skijumping\", \"skijumper\", \"skiing\", \"skateboard\", \"sisyphus\",\n",
        "    \"sinks\", \"sink\", \"singer\", \"sine\", \"sillywalks\", \"simba\", \"sign\", \"shrubbery\", \"shops\",\n",
        "    \"shopping\", \"shoot\", \"sherlockholmes\", \"shield\", \"ships\", \"shoes\", \"shell\", \"sheep\",\n",
        "    \"sharks\", \"shamrock\", \"shadow\", \"sewing\", \"settee\", \"sesamestreet\", \"seashell\", \"seascape\",\n",
        "    \"seal\", \"seahorse\", \"sea\", \"scream\", \"scroll\", \"scuba\", \"scorpion\", \"scooter\", \"scissors\",\n",
        "    \"science\", \"scales\", \"scarecrow\", \"sax\", \"saw\", \"saturn\", \"satellites\", \"sandwich\", \"samoa\",\n",
        "    \"samurai\", \"sailplane\", \"sailboat\", \"sail\", \"running\", \"sack\", \"rowing\", \"romance\",\n",
        "    \"romeoandjuliet\", \"room\", \"rooster\", \"rose\", \"rollerskates\", \"rollercoaster\", \"rockinghorse\",\n",
        "    \"roadrunner\", \"roadworks\", \"robin\", \"robot\", \"ring\", \"rhino\", \"restaurant\", \"reindeer\",\n",
        "    \"relax\", \"recycle\", \"recordplayer\", \"raven\", \"raygun\", \"reading\", \"rapunzel\", \"rainbow\",\n",
        "    \"radiotelescope\", \"quill\", \"quiet\", \"queue\", \"yuck\", \"ymca\", \"yingyang\", \"wstmnstr\", \"wrestle\",\n",
        "    \"wrench\", \"wormhole\", \"woodpecker\", \"wombat\", \"windsurfing\", \"whitetower\", \"whitehse\",\n",
        "    \"wheelbarrow\", \"wheel\", \"westminsterabby\", \"werewolf\", \"weasel\", \"wastedisposal\", \"wallpaper\",\n",
        "    \"walkietalkie\", \"waffle\", \"vine\", \"victorian\", \"vespa\", \"vase\", \"vancouver\", \"van\", \"usb\",\n",
        "    \"universe\", \"unihannover\", \"unhappy\", \"typewriter\", \"tunnel\", \"tuba\", \"tshirt\", \"trousers\",\n",
        "    \"trolly\", \"triomphe\", \"triangle\", \"treasuremap\", \"trashcan\", \"trap\", \"transam\", \"trabant\",\n",
        "    \"towerofhanoi\", \"tournament\", \"touchpad\", \"toronto\", \"torch\", \"toothbrush\", \"tintin\",\n",
        "    \"tie\", \"thought\", \"thermos\", \"terminator\", \"tear\", \"tasmaniandevil\", \"tapir\", \"taxi\",\n",
        "    \"takeoff\", \"tachometer\", \"table\", \"swordfish\", \"switzerland\", \"swing\", \"sweden\",\n",
        "    \"surf\", \"sunshade\", \"suitcase\", \"stpetersburg\", \"stethoscope\", \"stereo\", \"steps\",\n",
        "    \"steamroller\", \"steamboat\", \"steamengine\", \"stapler\", \"srilanka\", \"squid\", \"spinningwheel\",\n",
        "    \"spiral\", \"spitfire\", \"splash\", \"spock\", \"spoon\", \"sparrow\", \"space\", \"snowmobile\",\n",
        "    \"sneakers\", \"sloth\", \"slipper\", \"sleigh\", \"sleepingbeauty\", \"skipping\", \"skating\",\n",
        "    \"silo\", \"silverware\", \"shuttle\", \"seagull\", \"seaplane\", \"seatle\", \"scoobydoo\",\n",
        "    \"scater\", \"scarf\", \"scandinavia\", \"sandcastle\", \"saltpepper\", \"salmon\", \"safetypin\",\n",
        "    \"sacrecouer\", \"ruler\", \"rugby\", \"rubik\", \"rope\", \"rollingpin\", \"rockingchair\", \"rikscha\",\n",
        "    \"refrigerator\", \"rattlesnake\", \"radiation\", \"radar\", \"quarter\", \"queen\", \"questionmark\",\n",
        "    \"pteranodon\", \"prize\", \"pram\", \"prayingmantis\", \"presskey\", \"pretzel\", \"pot\", \"possum\",\n",
        "    \"pointer\", \"pocketwatch\", \"plug\", \"playstation\", \"piranha\", \"pinwheel\", \"pillow\", \"pile\",\n",
        "    \"piccolo\", \"person\", \"peru\", \"pepper\", \"pentagon\", \"pencil\", \"pear\", \"payphone\", \"pasta\",\n",
        "    \"parthenon\", \"parcel\", \"parakeet\", \"parabola\", \"palace\", \"oyster\", \"organ\", \"ornament\",\n",
        "    \"onion\", \"opel\", \"omega\", \"observatory\", \"objects\", \"oboe\", \"maple\", \"maya\", \"musicstand\",\n",
        "    \"megacerops\", \"mallard\", \"neptune\", \"mittens\", \"nba\", \"metallica\", \"marimba\", \"meatball\",\n",
        "    \"mtv\", \"nbc\", \"netscape\", \"norway\", \"music\", \"microwave\", \"medal\", \"newspaper\",\n",
        "    \"abbey\", \"aborigine\", \"acorn\", \"acropolis\", \"russianadmiralty\", \"adventwreath\", \"aeroflot\",\n",
        "    \"airportrunway\", \"aker\", \"aladdin\", \"albatross\", \"alicecooper\", \"altar\", \"amoeba\",\n",
        "    \"anomalocaris\", \"answeringmachine\", \"antarctica\", \"antlers\", \"anvil\", \"arbor\",\n",
        "    \"armadillos\", \"artgallery\", \"award\", \"backup\", \"bacon\", \"badger\", \"ballista\",\n",
        "    \"banjo\", \"baphomet\", \"barbedwire\", \"barber\", \"barn\", \"bassinet\", \"bauhauslogo\",\n",
        "    \"bay\", \"germanbeacon\", \"beard\", \"bearhug\", \"beast\", \"bedpan\", \"beehive\", \"beretta\",\n",
        "    \"bib\", \"bible\", \"binoculars\", \"birdcage\", \"bjorklogo\", \"bleedinghearts\", \"blossom\",\n",
        "    \"blowfish\", \"bluebells\", \"boing\", \"bookshelf\", \"bookworm\", \"bosniamap\", \"bottleopener\",\n",
        "    \"bow\", \"box\", \"brachiopod\", \"braille\", \"bread\", \"broom\", \"injured\", \"budapest\", \"buggy\",\n",
        "    \"bulldog\", \"bulldozer\", \"flowerbunch\", \"bungee\", \"bungeejumping\", \"bunsenburner\", \"cabbage\",\n",
        "    \"cabin\", \"cabinet\", \"cadillac\", \"can\", \"candleholder\", \"capitol\", \"captainfuture\", \"caracal\",\n",
        "    \"caveman\", \"cbradio\", \"jacquescousteau\", \"jailcell\", \"cello\", \"cemeterygate\", \"centipede\",\n",
        "    \"cerberus\", \"cereal\", \"chanukiah\", \"chaosarrow\", \"charliebrown\", \"charon\", \"cherry\", \"chilipepper\",\n",
        "    \"computerchip\", \"chisel\", \"citadel\", \"citroen\", \"cliff\", \"clip\", \"cnn\", \"co2\", \"coachcar\",\n",
        "    \"cobra\", \"colibri\", \"colombiamap\", \"compostpit\", \"confucious\", \"corkscrew\", \"corridor\",\n",
        "    \"cottage\", \"eatingcracker\", \"cranejaws\", \"crayons\", \"creek\", \"crinoids\", \"crisps\",\n",
        "    \"croatiamap\", \"crossbow\", \"crutch\", \"officecubicle\", \"cupcake\", \"cycling\", \"cylinders\",\n",
        "    \"dazed\", \"ddrgermany\", \"deathbed\", \"deathstar\", \"housedecoration\", \"desk\", \"desktop\",\n",
        "    \"dictionary\", \"hauntedmansion\", \"soapdispenser\", \"dither\", \"dodecahedron\", \"dollarbill\",\n",
        "    \"donut\", \"doomgame\", \"dortmunduniversity\", \"dots\", \"dowel\", \"dreamcatcher\", \"dromedaries\",\n",
        "    \"dungeon\", \"earmuffs\", \"earthworm\", \"eggshell\", \"eskimo\", \"ethanol\", \"evidencefile\", \"ewe\",\n",
        "    \"falling\", \"fastfood\", \"faucet\", \"minifigure\", \"filecabinet\", \"fingerboard\", \"fireside\",\n",
        "    \"fishwheel\", \"flashlight\", \"floorplan\", \"flucare\", \"flyfishing\", \"flyswatter\", \"food\",\n",
        "    \"hackysack\", \"fortress\", \"paintingframe\", \"france\", \"frankenstein\", \"friends\", \"mountfuji\",\n",
        "    \"funeral\", \"galleon\", \"galley\", \"gameboy\", \"gasolinepump\", \"gavel\", \"sickbed\", \"geisha\",\n",
        "    \"gift\", \"goblet\", \"goblin\", \"gong\", \"grasshopper\", \"grave\", \"circulargrid\", \"grill\",\n",
        "    \"grin\", \"grinch\", \"h2o\", \"hacker\", \"halberd\", \"halfpipe\", \"hallway\", \"hallways\",\n",
        "    \"hamburg\", \"hammock\", \"handdoll\", \"grenade\", \"handicap\", \"handkerchief\", \"hanggliding\",\n",
        "    \"harmonica\", \"heelflip\", \"hibiscus\", \"hills\", \"hollywoodsign\", \"homersimpson\", \"horizon\",\n",
        "    \"horn\", \"horseshoe\", \"hostess\", \"hotwaterbottle\", \"hungarymap\", \"husbandwife\", \"hydrant\",\n",
        "    \"hyena\", \"igor\", \"iguana\", \"incapyramid\", \"infoicon\", \"injury\", \"ink\", \"intellogo\",\n",
        "    \"ironingboard\", \"jailroom\", \"janus\", \"javelin\", \"utahjazzlogo\", \"jfk\", \"jugs\", \"juicebox\",\n",
        "    \"jukebox\", \"jungle\", \"justicia\", \"kelloggsbox\", \"kermit\", \"kettle\", \"keyhole\", \"kickboard\",\n",
        "    \"kingkong\", \"kitten\", \"kleenex\", \"kleinbottle\", \"knives\", \"koeln\", \"kremlin\", \"kungfu\",\n",
        "    \"lake\", \"lamprey\", \"icelance\", \"landrover\", \"laurelwreath\", \"frogleap\", \"led\", \"leek\",\n",
        "    \"lemonsqeezer\", \"leprechaun\", \"lever\", \"library\", \"lilies\", \"livingroom\", \"lottery\",\n",
        "    \"lotus\", \"lounge\", \"lucia\", \"luckyluke\", \"lyre\"]\n",
        "print(len(classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I99V1MtzerFu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9vDQXY8e9Ua"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmwMt_05W_TD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def save_ascii_art(url, metadata, base_dir, wanted_classes=None):\n",
        "    # Parse the URL to create a sub-directory name\n",
        "    category = url.split('/')[-1]\n",
        "\n",
        "    # Check if category is not in wanted_classes, skip processing\n",
        "    if wanted_classes and category not in wanted_classes:\n",
        "        return\n",
        "\n",
        "    # Separate directories for TXT and PNG files within each category\n",
        "    txt_dir = os.path.join(base_dir, category, 'TXT')\n",
        "    png_dir = os.path.join(base_dir, category, 'PNG')\n",
        "    os.makedirs(txt_dir, exist_ok=True)\n",
        "    os.makedirs(png_dir, exist_ok=True)\n",
        "\n",
        "    # Request and parse the webpage\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    ASCII_images = soup.find_all('pre')\n",
        "\n",
        "    # Assume the existence of necessary helper functions like split_pre_tag_content, etc.\n",
        "\n",
        "    # Process ASCII art\n",
        "    ascii_arts = split_pre_tag_content(ASCII_images[0].get_text())\n",
        "    ascii_arts = filter_by_line_count(ascii_arts)\n",
        "\n",
        "    for idx, ascii_art in enumerate(ascii_arts):\n",
        "        original_art=ascii_art\n",
        "        further_processing_needed = True\n",
        "\n",
        "        ascii_art = remove_artist_tag(ascii_art)\n",
        "        if ascii_art != original_art:\n",
        "          further_processing_needed = False\n",
        "\n",
        "        if further_processing_needed:\n",
        "          ascii_art = remove_unknown_artist_tag(ascii_art)\n",
        "\n",
        "        if ascii_art != original_art:\n",
        "          further_processing_needed = False\n",
        "\n",
        "        if further_processing_needed:\n",
        "          ascii_art = remove_right_tag(ascii_art)\n",
        "\n",
        "        if ascii_art != original_art:\n",
        "            further_processing_needed = False\n",
        "\n",
        "        if further_processing_needed:\n",
        "          ascii_art = remove_right_tag(ascii_art)\n",
        "\n",
        "        if ascii_art != original_art:\n",
        "          further_processing_needed = False\n",
        "\n",
        "        if further_processing_needed:\n",
        "          ascii_art = remove_creator_tag_pattern(ascii_art)\n",
        "\n",
        "        if ascii_art != original_art:\n",
        "          further_processing_needed = False\n",
        "\n",
        "        if further_processing_needed:\n",
        "          ascii_art = remove_bracket_or_dash_enclosed_tags(ascii_art)\n",
        "\n",
        "        if ascii_art != original_art:\n",
        "          further_processing_needed = False\n",
        "\n",
        "\n",
        "          ascii_art = remove_dates_from_ascii_art(ascii_art)\n",
        "          ascii_art = replace_emails_in_ascii_image(ascii_art)\n",
        "\n",
        "        # Save ASCII art to TXT file\n",
        "        txt_file_name = f'{category}_{idx:03}.txt'\n",
        "        txt_file_path = os.path.join(txt_dir, txt_file_name)\n",
        "        with open(txt_file_path, 'w') as file:\n",
        "            file.write(ascii_art)\n",
        "\n",
        "        # TODO: Convert ASCII art to PNG and save (if applicable)\n",
        "        # png_file_name = f'{category}_{idx:03}.png'\n",
        "        # png_file_path = os.path.join(png_dir, png_file_name)\n",
        "        # Convert and save PNG file here...\n",
        "\n",
        "        # Update metadata\n",
        "        metadata.append({\n",
        "            'file_name': txt_file_name,\n",
        "            'class': category,\n",
        "            'unique_id': f'{category}_{idx:03}',\n",
        "            'url': url\n",
        "        })\n",
        "\n",
        "    print(f\"Saved ASCII art and metadata for {url}\")\n",
        "\n",
        "# Base directory for saving ASCII art\n",
        "base_dir = '/content/drive/My Drive/ASCII_Art_Dataset_Final_Uncleaned'\n",
        "\n",
        "# Master metadata list\n",
        "metadata = []\n",
        "\n",
        "# Example usage\n",
        "for url in urls:\n",
        "  save_ascii_art('https://ascii.co.uk' + url, metadata, base_dir, classes)\n",
        "\n",
        "# After all URLs are processed, save the master metadata file\n",
        "jsonl_metadata_path = os.path.join(base_dir, 'master_metadata.jsonl')\n",
        "with open(jsonl_metadata_path, 'w') as jsonl_file:\n",
        "    for item in metadata:\n",
        "        jsonl_file.write(json.dumps(item) + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymKnLT9SEYMg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def is_ascii(s):\n",
        "    try:\n",
        "        s.encode('ascii')\n",
        "        return True\n",
        "    except UnicodeEncodeError:\n",
        "        return False\n",
        "\n",
        "def delete_and_renumber_files(base_dir): # Removes non-acsii art, like the first few english sentences in abacus. And also those that use non-ascii characters.\n",
        "    # Loop through each category directory\n",
        "    for category in os.listdir(base_dir):\n",
        "        category_path = os.path.join(base_dir, category)\n",
        "        if not os.path.isdir(category_path):\n",
        "            continue  # Skip if it's not a directory\n",
        "\n",
        "        txt_dir = os.path.join(category_path, 'TXT')\n",
        "        if not os.path.exists(txt_dir):\n",
        "            continue  # Skip if TXT directory doesn't exist\n",
        "\n",
        "        # List of files to keep\n",
        "        files_to_keep = []\n",
        "\n",
        "        # Check each file in the TXT directory\n",
        "        for file_name in sorted(os.listdir(txt_dir)):\n",
        "            file_path = os.path.join(txt_dir, file_name)\n",
        "            if os.path.isfile(file_path):\n",
        "                with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                    content = file.read()\n",
        "                    if len(content.splitlines()) >= 3 and is_ascii(content):\n",
        "                        files_to_keep.append(file_path)\n",
        "\n",
        "        # Delete files that are not in files_to_keep\n",
        "        for file_name in os.listdir(txt_dir):\n",
        "            file_path = os.path.join(txt_dir, file_name)\n",
        "            if file_path not in files_to_keep:\n",
        "                print(file_name)\n",
        "                os.remove(file_path)\n",
        "\n",
        "        # Renumber remaining files\n",
        "        for new_index, file_path in enumerate(files_to_keep):\n",
        "            new_file_name = f'{category}_{new_index:03}.txt'\n",
        "            new_file_path = os.path.join(txt_dir, new_file_name)\n",
        "            os.rename(file_path, new_file_path)\n",
        "\n",
        "# Example usage\n",
        "delete_and_renumber_files(base_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s26T-8lQpufx"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageDraw, ImageFont, ImageFilter\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_ascii_for_clip(ascii_art, font_path=\"Courier.ttf\", font_size=12):\n",
        "    # Create an image from ASCII art\n",
        "    try:\n",
        "        # Attempt to use a specific font\n",
        "        font = ImageFont.truetype(\"Courier.ttf\", size=font_size)\n",
        "    except IOError:\n",
        "        # Fallback to the default PIL font if specific font is not found\n",
        "        font = ImageFont.load_default()\n",
        "    image = Image.new('RGB', (800, 600), color=(0, 0, 0))  # Create a black background image\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    draw.text((10, 10), ascii_art, fill=(255, 255, 255), font=font)  # White text on black background\n",
        "\n",
        "    # Crop to remove unnecessary whitespace\n",
        "    bbox = image.getbbox()\n",
        "    cropped_image = image.crop(bbox) if bbox else image\n",
        "\n",
        "    # Convert to grayscale\n",
        "    grayscale_image = cropped_image.convert(\"L\")\n",
        "\n",
        "    # Apply a slight blur\n",
        "    blurred_image = grayscale_image.filter(ImageFilter.GaussianBlur(1))\n",
        "\n",
        "    # Normalize the pixel values if required by the model (e.g., scale between 0 and 1)\n",
        "    # Here, we assume a normalization of 0 to 1 for grayscale images\n",
        "    normalized_image = np.array(blurred_image) / 255.0\n",
        "\n",
        "    return Image.fromarray((normalized_image * 255).astype(np.uint8))\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def process_all_ascii_files(base_dir): # Loops through the files and creates images using the above function for each.\n",
        "    # Iterate over all category directories in the base directory\n",
        "    for category_dir in Path(base_dir).iterdir():\n",
        "        if category_dir.is_dir():\n",
        "            txt_dir = category_dir / 'TXT'\n",
        "            png_dir = category_dir / 'PNG'\n",
        "\n",
        "            # Create PNG directory if it doesn't exist\n",
        "            png_dir.mkdir(exist_ok=True)\n",
        "\n",
        "            # Process each text file in the TXT directory\n",
        "            for text_file in txt_dir.glob(\"*.txt\"):\n",
        "                with open(text_file, 'r', encoding='utf-8') as file:  # Specify UTF-8 encoding\n",
        "                    ascii_art = file.read()\n",
        "                    # Preprocess the ASCII art\n",
        "                    print(text_file)\n",
        "                    preprocessed_image = preprocess_ascii_for_clip(ascii_art)\n",
        "                    # Save the processed image in the PNG directory\n",
        "                    image_file = png_dir / text_file.with_suffix('.png').name\n",
        "                    preprocessed_image.save(image_file)\n",
        "\n",
        "# Base directory of your ASCII art dataset\n",
        "\n",
        "# Process all ASCII art files\n",
        "process_all_ascii_files(base_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlvN-XJopUbl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIP__kf3bncA"
      },
      "outputs": [],
      "source": [
        "len(urls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBQ6N-HVhPL2"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "def process_ascii_files_and_generate_metadata(base_dir):\n",
        "    metadata = []\n",
        "\n",
        "    # Loop through each category directory\n",
        "    for category in os.listdir(base_dir):\n",
        "        category_path = os.path.join(base_dir, category)\n",
        "        if not os.path.isdir(category_path):\n",
        "            continue  # Skip if it's not a directory\n",
        "\n",
        "        txt_dir = os.path.join(category_path, 'TXT')\n",
        "        if not os.path.exists(txt_dir):\n",
        "            continue  # Skip if TXT directory doesn't exist\n",
        "\n",
        "        # Process each file in the TXT directory\n",
        "        for file_name in sorted(os.listdir(txt_dir)):\n",
        "            file_path = os.path.join(txt_dir, file_name)\n",
        "            if os.path.isfile(file_path):\n",
        "                with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                    content = file.read()\n",
        "\n",
        "                    # Create metadata entry\n",
        "                    unique_id = f\"{category}_{file_name.split('.')[0]}\"\n",
        "                    metadata_entry = {\n",
        "                        'class': category,\n",
        "                        'unique_id': unique_id,\n",
        "                        'file_name': file_name,\n",
        "                        'ascii_art': content\n",
        "                    }\n",
        "                    metadata.append(metadata_entry)\n",
        "\n",
        "    # Save metadata to a master JSON Lines file\n",
        "    jsonl_metadata_path = os.path.join(base_dir, 'master_metadata.jsonl')\n",
        "    with open(jsonl_metadata_path, 'w') as jsonl_file:\n",
        "        for item in metadata:\n",
        "            jsonl_file.write(json.dumps(item) + '\\n')\n",
        "\n",
        "# Example usage\n",
        "process_ascii_files_and_generate_metadata(base_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQXAVglQr_tY"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Path to your JSON Lines file in Colab environment\n",
        "jsonl_file_path = '/content/drive/My Drive/ASCII_Art_Dataset_Final_Uncleaned/master_metadata.jsonl'\n",
        "\n",
        "# Trigger download to your local computer\n",
        "files.download(jsonl_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVYA5nWl008P"
      },
      "outputs": [],
      "source": [
        "def filter_jsonl_by_classes(input_file_path, output_file_path, wanted_classes):\n",
        "    # Read the JSON Lines file and filter it\n",
        "    filtered_data = []\n",
        "\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            data = json.loads(line)\n",
        "            if data['class'] in wanted_classes:\n",
        "                filtered_data.append(data)\n",
        "\n",
        "    # Save the filtered data to a new JSON Lines file\n",
        "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
        "        for item in filtered_data:\n",
        "            jsonl_line = json.dumps(item)\n",
        "            file.write(jsonl_line + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmyoHqO31_6K"
      },
      "outputs": [],
      "source": [
        "# Original JSON Lines file path in Google Drive\n",
        "input_jsonl_file = '/content/drive/My Drive/ASCII_Art_Dataset_Final_Uncleaned/master_metadata.jsonl'\n",
        "\n",
        "# Path for the output filtered JSON Lines file in Google Drive\n",
        "output_jsonl_file = '/content/drive/My Drive/ASCII_Art_Dataset_Final_Uncleaned/filtered_metadata.jsonl'\n",
        "# Assuming 'classes' variable contains the list of wanted classes\n",
        "filter_jsonl_by_classes(input_jsonl_file, output_jsonl_file, classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S5R9Hfdwb9w"
      },
      "source": [
        "### After creating the dataset, test to see if it works below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btkxjUu_tB3S"
      },
      "outputs": [],
      "source": [
        "# Example usage, using the google drive file structure. Reached quota limit from google drive, so errors out.\n",
        "\n",
        "# import os\n",
        "# from pathlib import Path\n",
        "# from PIL import Image\n",
        "# import torch\n",
        "# from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# # Load CLIP model and processor\n",
        "# model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# def calculate_clip_similarity(image1, image2):\n",
        "#     # Process images with CLIP processor\n",
        "#     inputs = processor(images=[image1, image2], return_tensors=\"pt\", padding=True)\n",
        "\n",
        "#     # Perform inference without tracking gradients\n",
        "#     with torch.no_grad():\n",
        "#         image_features = model.get_image_features(**inputs)\n",
        "\n",
        "#     # Calculate similarity (cosine similarity)\n",
        "#     cosine_similarity = torch.nn.functional.cosine_similarity(image_features[0].unsqueeze(0), image_features[1].unsqueeze(0), dim=1)\n",
        "#     return cosine_similarity.item()\n",
        "\n",
        "# base_dir = '/content/drive/My Drive/ASCII_Art_Dataset_Final_Uncleaned'  # Update as per your directory structure\n",
        "\n",
        "# # List all image files\n",
        "# image_files = [file for file in Path(base_dir).rglob('*.png')]\n",
        "\n",
        "# print(len(image_files))\n",
        "# # Initialize a similarity matrix\n",
        "# num_images = len(image_files)\n",
        "# similarity_matrix = torch.zeros((num_images, num_images))\n",
        "\n",
        "# # Calculate CLIP similarities and fill the matrix\n",
        "# for i in range(num_images):\n",
        "#     for j in range(num_images):\n",
        "#         if i != j:  # Skip comparison with itself\n",
        "#             with Image.open(image_files[i]) as image1, Image.open(image_files[j]) as image2:\n",
        "#                 similarity = calculate_clip_similarity(image1, image2)\n",
        "#                 similarity_matrix[i, j] = similarity\n",
        "\n",
        "# # Print the similarity matrix\n",
        "# print(similarity_matrix)\n",
        "\n",
        "\n",
        "# Example usage from jsonlines\n",
        "\n",
        "import json\n",
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image, ImageDraw, ImageFont, ImageFilter\n",
        "import numpy as np\n",
        "\n",
        "# Load CLIP model and processor\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "def calculate_clip_similarity(image1, image2):\n",
        "    # Process images with CLIP processor\n",
        "    inputs = processor(images=[image1, image2], return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # Perform inference without tracking gradients\n",
        "    with torch.no_grad():\n",
        "        image_features = model.get_image_features(**inputs)\n",
        "\n",
        "    # Calculate similarity (cosine similarity)\n",
        "    cosine_similarity = torch.nn.functional.cosine_similarity(image_features[0].unsqueeze(0), image_features[1].unsqueeze(0), dim=1)\n",
        "    return cosine_similarity.item()\n",
        "\n",
        "\n",
        "\n",
        "# Path to your JSON Lines file\n",
        "jsonl_file_path = '/content/drive/My Drive/ASCII_Art_Dataset_Final_Uncleaned/master_metadata.jsonl'\n",
        "\n",
        "# Read ASCII art from JSON Lines file\n",
        "ascii_arts = []\n",
        "with open(jsonl_file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        data = json.loads(line)\n",
        "        ascii_arts.append(data['ascii_art'])\n",
        "\n",
        "# Initialize a similarity matrix\n",
        "num_images = len(ascii_arts)\n",
        "similarity_matrix = torch.zeros((num_images, num_images))\n",
        "\n",
        "# Calculate CLIP similarities and fill the matrix\n",
        "for i in range(num_images):\n",
        "    for j in range(num_images):\n",
        "        if i != j:  # Skip comparison with itself\n",
        "            image1 = preprocess_ascii_for_clip(ascii_arts[i])\n",
        "            image2 = preprocess_ascii_for_clip(ascii_arts[j])\n",
        "            similarity = calculate_clip_similarity(image1, image2)\n",
        "            similarity_matrix[i, j] = similarity\n",
        "\n",
        "# Print the similarity matrix\n",
        "print(similarity_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1MXACx5u1Vb"
      },
      "outputs": [],
      "source": [
        "print(classes.length())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}